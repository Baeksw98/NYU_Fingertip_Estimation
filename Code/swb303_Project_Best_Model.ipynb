{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00389509-1ace-4a39-ba85-9b975c1a16b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Intro to ML Capstone Project \n",
    "* Professor: Lerrel Pinto\n",
    "* Made by Sangwon Baek\n",
    "* December 10th 2022\n",
    "* Kaggle Site URL:\n",
    "https://www.kaggle.com/competitions/csci-ua-473-intro-to-machine-learning-fall22/overview\n",
    "* Batchsize 32 was used\n",
    "* ResNet 152 / LR : 0.001 (1e-3)\n",
    "* Applied normalization for RGB by calculating them / for depth(divided by 1000 first) used CV2 normalizer minmax\n",
    "* Test/Valid split (8:2), implemented earlystopping (patience=5, stepLR on every epoch with Gamma 0.8 step 1)\n",
    "* Did (targets) ground truth * 100 before passing to the loss function (meters to mm) then divided by 100 before submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320f55b2-118e-47c5-b769-d67fd9e21b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet50, ResNet50_Weights, resnet18, ResNet18_Weights, resnet152, ResNet152_Weights\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle as pkl "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d54fd-dbec-4da2-a0d1-ca6a5d6da029",
   "metadata": {},
   "source": [
    "### Preprocessing and loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75daed85-b3ef-4cd7-a243-d0099e566985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LazyLoadDataset(Dataset):\n",
    "    def __init__(self, path, train=True, transform=None):\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.transform_0 = None\n",
    "        self.transform_1 = None\n",
    "        self.transform_2 = None\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            self.transform_0 = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(240),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Normalize([0.4352, 0.4170, 0.3960], [0.1992, 0.1987, 0.2111])\n",
    "            ])\n",
    "            self.transform_1 = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(240),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Normalize([0.5008, 0.4879, 0.4697], [0.2276, 0.2252, 0.2417])\n",
    "            ])\n",
    "            self.transform_2 = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(240),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Normalize([0.5193, 0.4820, 0.4412], [0.2293, 0.2288, 0.2465])\n",
    "            ])\n",
    "        path = path + (\"train/\" if train else \"test/\")\n",
    "        \n",
    "        self.pathX = path+\"X/\"\n",
    "        self.pathY = path+\"Y/\"\n",
    "        \n",
    "        self.data = os.listdir(self.pathX)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        f = self.data[idx]\n",
    "        \n",
    "        #Read rgb images\n",
    "        img0 = cv2.imread(self.pathX + f + '/rgb/0.png')\n",
    "        img1 = cv2.imread(self.pathX + f + '/rgb/1.png')\n",
    "        img2 = cv2.imread(self.pathX + f + '/rgb/2.png')\n",
    "        \n",
    "        #read depth images\n",
    "        depth = np.load(self.pathX + f + '/depth.npy')        \n",
    "        depth = depth/1000\n",
    "        \n",
    "        #Convert RGB and depth images to tensor\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform_0(img0)\n",
    "            img1 = self.transform_1(img1)\n",
    "            img2 = self.transform_2(img2)\n",
    "        \n",
    "        #Perform transformation on Depth image\n",
    "        depth = cv2.normalize(depth, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)  \n",
    "            \n",
    "        #read field ID & Y\n",
    "        field_id = pkl.load(open(self.pathX + f + '/field_id.pkl', 'rb'))\n",
    "        \n",
    "        if self.train==True:\n",
    "            Y = np.load(self.pathY + f + '.npy')\n",
    "            return (img0, img1, img2, depth, field_id), Y\n",
    "        if self.train==False:\n",
    "            return (img0, img1, img2, depth, field_id)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b3c4f-ebe7-4ae4-9634-6a870968773c",
   "metadata": {},
   "source": [
    "### Explore data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9302fab7-3a34-4ed5-bc19-e22726075c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform to tensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "#Lazy Load the dataset\n",
    "dataset = LazyLoadDataset('../lazydata/',train=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99a7e63-fdd4-49b2-ad12-a6ae487c89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define train/validation size (8:2)\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "validation_size = len(dataset)-train_size\n",
    "\n",
    "#Randomly split dataset into train and validation dataset with specified size above\n",
    "train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])\n",
    "\n",
    "#Create train/validation dataloader with batch_size of 64, 32 respectively\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed25f2be-f3e5-40a7-ba88-8f1a969e9721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 2716 \n",
      "Train Loader size: 85\n",
      "Validation set size: 680 \n",
      "Validation Loader size: 22\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set size: {} \\nTrain Loader size: {}\".format(len(train_dataset),len(train_dataloader)))\n",
    "print(\"Validation set size: {} \\nValidation Loader size: {}\".format(len(validation_dataset),len(validation_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14c86a20-95ea-4451-97ba-22af68da852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "img0 shapetorch.Size([32, 3, 224, 224])\n",
      "img1 shapetorch.Size([32, 3, 224, 224])\n",
      "img2 shapetorch.Size([32, 3, 224, 224])\n",
      "depth shapetorch.Size([32, 3, 224, 224])\n",
      "field id ('3617', '3175', '2547', '2973', '2878', '1370', '1478', '1040', '4005', '1758', '251', '1856', '2774', '160', '3816', '1977', '1231', '698', '3339', '516', '892', '1049', '3471', '2712', '2818', '2187', '3036', '502', '1943', '1749', '710', '1781')\n",
      "labels size torch.Size([32, 12])\n"
     ]
    }
   ],
   "source": [
    "for i, ((img0, img1, img2, depth, field_id), labels) in enumerate(train_dataloader):\n",
    "    print(i)\n",
    "    # print(depth[0])\n",
    "    print(\"img0 shape{}\".format(img0.size()))\n",
    "    print(\"img1 shape{}\".format(img1.size()))\n",
    "    print(\"img2 shape{}\".format(img2.size()))\n",
    "    print(\"depth shape{}\".format(depth.shape))\n",
    "    print(\"field id {}\".format(field_id))\n",
    "    print(\"labels size {}\".format(labels.size()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d0c33-8d4f-47df-be2a-745969639562",
   "metadata": {},
   "source": [
    "### Set device to cuda & Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bace9165-a20e-4cf9-97b5-5dda0f5b536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 12\n",
    "lr = 1e-3\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afafd8d-3cd0-44ab-89f8-9cac37a75b92",
   "metadata": {},
   "source": [
    "### Train 1st img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b0f6b5-30d1-492f-b085-955f54f6d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_1(epoch, model, optimizer, scheduler):\n",
    "    # Early stopping Parameters\n",
    "    Best_Val_loss = None\n",
    "    patience = 5\n",
    "    trigger_times = 0\n",
    "    for epoch in range(0, epochs):\n",
    "        model.train()\n",
    "        for batch_idx, ((img0, img1, img2, depth, field_id), target) in enumerate(train_dataloader): \n",
    "            #Stack all three RGB \n",
    "            img_all = torch.stack((img0[:,0,:,:],img0[:,1,:,:],img0[:,2,:,:]),1)\n",
    "            data = img_all\n",
    "            data = data.to(device)\n",
    "\n",
    "            #Send to Device\n",
    "            target = target.to(device)\n",
    "            \n",
    "            #multiply GT by 100\n",
    "            target = target*100\n",
    "            \n",
    "            #Run necessary train implementations\n",
    "            output = model(data)\n",
    "            \n",
    "            #Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #forward and backward propagation\n",
    "            loss_function = nn.MSELoss()\n",
    "            #Root Mean Squared Error (RMSE) \n",
    "            train_loss = torch.sqrt(loss_function(output.float(), target.float()))\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #Show progress\n",
    "            if batch_idx % 28 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "                    100. * batch_idx / len(train_dataloader), train_loss.item()))\n",
    "   \n",
    "        #To help model improving apply scheduler to reduce learning rate\n",
    "        scheduler.step()\n",
    "        curr_lr = scheduler.get_last_lr()\n",
    "        print(\"current lr{}\".format(curr_lr))     \n",
    "        \n",
    "        #Early Stopping\n",
    "        Curr_Val_loss = validation_1(model)\n",
    "        print('Valid Epoch: {} Current Validation Loss: {:.6F}'.format(epoch, Curr_Val_loss))\n",
    "        \n",
    "        if Curr_Val_loss < 0.242:\n",
    "            print (\"Best model obtained! Set Patience = 2\")\n",
    "            patience = 2 \n",
    "        \n",
    "        if Best_Val_loss is None:\n",
    "            Best_Val_loss = Curr_Val_loss - 0.0001\n",
    "        #Model didn't improve so add trigger time\n",
    "        if Curr_Val_loss > Best_Val_loss:\n",
    "            trigger_times += 1\n",
    "            print('trigger times:', trigger_times)\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                return model\n",
    "\n",
    "        #Reset to trigger_time 0 and update best validation loss\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "            Best_Val_loss = Curr_Val_loss\n",
    "            print('trigger times: 0 Best_Val_loss: {:.6F}'.format(Best_Val_loss)) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be73e5b9-d3da-4b53-9d55-876f62bd5f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_1(model):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, ((img0, img1, img2, depth, field_id), target) in enumerate(validation_dataloader):\n",
    "            #Stack all three RGB and depth images\n",
    "            img_all = torch.stack((img0[:,0,:,:],img0[:,1,:,:],img0[:,2,:,:]),1)\n",
    "            data = img_all\n",
    "            data = data.to(device)\n",
    "            \n",
    "            #Send to Device\n",
    "            target = target.to(device)\n",
    "            \n",
    "            #multiply GT by 100\n",
    "            target = target*100\n",
    "            \n",
    "            output = model(data)\n",
    "            loss_function = nn.MSELoss()\n",
    "            loss = torch.sqrt(loss_function(output.float(), target.float()))\n",
    "            loss_total += loss.item()\n",
    "    return loss_total / len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e57f0886-8360-4005-a277-b506166cadcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/2716 (0%)]\tLoss: 6.830080\n",
      "Train Epoch: 0 [896/2716 (33%)]\tLoss: 0.993739\n",
      "Train Epoch: 0 [1792/2716 (66%)]\tLoss: 0.879711\n",
      "Train Epoch: 0 [2352/2716 (99%)]\tLoss: 0.763808\n",
      "current lr[0.0008]\n",
      "Valid Epoch: 0 Current Validation Loss: 0.797618\n",
      "trigger times: 1\n",
      "Train Epoch: 1 [0/2716 (0%)]\tLoss: 0.694618\n",
      "Train Epoch: 1 [896/2716 (33%)]\tLoss: 0.656592\n",
      "Train Epoch: 1 [1792/2716 (66%)]\tLoss: 0.621730\n",
      "Train Epoch: 1 [2352/2716 (99%)]\tLoss: 0.427999\n",
      "current lr[0.00064]\n",
      "Valid Epoch: 1 Current Validation Loss: 0.446688\n",
      "trigger times: 0 Best_Val_loss: 0.446688\n",
      "Train Epoch: 2 [0/2716 (0%)]\tLoss: 0.441339\n",
      "Train Epoch: 2 [896/2716 (33%)]\tLoss: 0.510876\n",
      "Train Epoch: 2 [1792/2716 (66%)]\tLoss: 0.417094\n",
      "Train Epoch: 2 [2352/2716 (99%)]\tLoss: 0.384095\n",
      "current lr[0.0005120000000000001]\n",
      "Valid Epoch: 2 Current Validation Loss: 0.445030\n",
      "trigger times: 0 Best_Val_loss: 0.445030\n",
      "Train Epoch: 3 [0/2716 (0%)]\tLoss: 0.526889\n",
      "Train Epoch: 3 [896/2716 (33%)]\tLoss: 0.286180\n",
      "Train Epoch: 3 [1792/2716 (66%)]\tLoss: 0.338825\n",
      "Train Epoch: 3 [2352/2716 (99%)]\tLoss: 0.346453\n",
      "current lr[0.0004096000000000001]\n",
      "Valid Epoch: 3 Current Validation Loss: 0.366323\n",
      "trigger times: 0 Best_Val_loss: 0.366323\n",
      "Train Epoch: 4 [0/2716 (0%)]\tLoss: 0.290016\n",
      "Train Epoch: 4 [896/2716 (33%)]\tLoss: 0.356367\n",
      "Train Epoch: 4 [1792/2716 (66%)]\tLoss: 0.387294\n",
      "Train Epoch: 4 [2352/2716 (99%)]\tLoss: 0.276040\n",
      "current lr[0.0003276800000000001]\n",
      "Valid Epoch: 4 Current Validation Loss: 0.296331\n",
      "trigger times: 0 Best_Val_loss: 0.296331\n",
      "Train Epoch: 5 [0/2716 (0%)]\tLoss: 0.257084\n",
      "Train Epoch: 5 [896/2716 (33%)]\tLoss: 0.277414\n",
      "Train Epoch: 5 [1792/2716 (66%)]\tLoss: 0.256286\n",
      "Train Epoch: 5 [2352/2716 (99%)]\tLoss: 0.240594\n",
      "current lr[0.0002621440000000001]\n",
      "Valid Epoch: 5 Current Validation Loss: 0.315297\n",
      "trigger times: 1\n",
      "Train Epoch: 6 [0/2716 (0%)]\tLoss: 0.252570\n",
      "Train Epoch: 6 [896/2716 (33%)]\tLoss: 0.231342\n",
      "Train Epoch: 6 [1792/2716 (66%)]\tLoss: 0.254641\n",
      "Train Epoch: 6 [2352/2716 (99%)]\tLoss: 0.250950\n",
      "current lr[0.00020971520000000012]\n",
      "Valid Epoch: 6 Current Validation Loss: 0.299679\n",
      "trigger times: 2\n",
      "Train Epoch: 7 [0/2716 (0%)]\tLoss: 0.210413\n",
      "Train Epoch: 7 [896/2716 (33%)]\tLoss: 0.191135\n",
      "Train Epoch: 7 [1792/2716 (66%)]\tLoss: 0.153840\n",
      "Train Epoch: 7 [2352/2716 (99%)]\tLoss: 0.191268\n",
      "current lr[0.0001677721600000001]\n",
      "Valid Epoch: 7 Current Validation Loss: 0.269646\n",
      "trigger times: 0 Best_Val_loss: 0.269646\n",
      "Train Epoch: 8 [0/2716 (0%)]\tLoss: 0.169798\n",
      "Train Epoch: 8 [896/2716 (33%)]\tLoss: 0.171329\n",
      "Train Epoch: 8 [1792/2716 (66%)]\tLoss: 0.170915\n",
      "Train Epoch: 8 [2352/2716 (99%)]\tLoss: 0.238843\n",
      "current lr[0.00013421772800000008]\n",
      "Valid Epoch: 8 Current Validation Loss: 0.256158\n",
      "trigger times: 0 Best_Val_loss: 0.256158\n",
      "Train Epoch: 9 [0/2716 (0%)]\tLoss: 0.155823\n",
      "Train Epoch: 9 [896/2716 (33%)]\tLoss: 0.209067\n",
      "Train Epoch: 9 [1792/2716 (66%)]\tLoss: 0.176416\n",
      "Train Epoch: 9 [2352/2716 (99%)]\tLoss: 0.209335\n",
      "current lr[0.00010737418240000007]\n",
      "Valid Epoch: 9 Current Validation Loss: 0.266990\n",
      "trigger times: 1\n",
      "Train Epoch: 10 [0/2716 (0%)]\tLoss: 0.192517\n",
      "Train Epoch: 10 [896/2716 (33%)]\tLoss: 0.122501\n",
      "Train Epoch: 10 [1792/2716 (66%)]\tLoss: 0.138331\n",
      "Train Epoch: 10 [2352/2716 (99%)]\tLoss: 0.147890\n",
      "current lr[8.589934592000007e-05]\n",
      "Valid Epoch: 10 Current Validation Loss: 0.245300\n",
      "trigger times: 0 Best_Val_loss: 0.245300\n",
      "Train Epoch: 11 [0/2716 (0%)]\tLoss: 0.143039\n",
      "Train Epoch: 11 [896/2716 (33%)]\tLoss: 0.145312\n",
      "Train Epoch: 11 [1792/2716 (66%)]\tLoss: 0.121197\n",
      "Train Epoch: 11 [2352/2716 (99%)]\tLoss: 0.139183\n",
      "current lr[6.871947673600006e-05]\n",
      "Valid Epoch: 11 Current Validation Loss: 0.246234\n",
      "trigger times: 1\n",
      "Train Epoch: 12 [0/2716 (0%)]\tLoss: 0.145875\n",
      "Train Epoch: 12 [896/2716 (33%)]\tLoss: 0.155783\n",
      "Train Epoch: 12 [1792/2716 (66%)]\tLoss: 0.146298\n",
      "Train Epoch: 12 [2352/2716 (99%)]\tLoss: 0.139291\n",
      "current lr[5.497558138880005e-05]\n",
      "Valid Epoch: 12 Current Validation Loss: 0.240988\n",
      "Best model obtained! Set Patience = 2\n",
      "trigger times: 0 Best_Val_loss: 0.240988\n",
      "Train Epoch: 13 [0/2716 (0%)]\tLoss: 0.120321\n",
      "Train Epoch: 13 [896/2716 (33%)]\tLoss: 0.136148\n",
      "Train Epoch: 13 [1792/2716 (66%)]\tLoss: 0.106503\n",
      "Train Epoch: 13 [2352/2716 (99%)]\tLoss: 0.112286\n",
      "current lr[4.3980465111040044e-05]\n",
      "Valid Epoch: 13 Current Validation Loss: 0.245378\n",
      "trigger times: 1\n",
      "Train Epoch: 14 [0/2716 (0%)]\tLoss: 0.114527\n",
      "Train Epoch: 14 [896/2716 (33%)]\tLoss: 0.109528\n",
      "Train Epoch: 14 [1792/2716 (66%)]\tLoss: 0.111332\n",
      "Train Epoch: 14 [2352/2716 (99%)]\tLoss: 0.129479\n",
      "current lr[3.5184372088832036e-05]\n",
      "Valid Epoch: 14 Current Validation Loss: 0.236438\n",
      "Best model obtained! Set Patience = 2\n",
      "trigger times: 0 Best_Val_loss: 0.236438\n",
      "Train Epoch: 15 [0/2716 (0%)]\tLoss: 0.091305\n",
      "Train Epoch: 15 [896/2716 (33%)]\tLoss: 0.126481\n",
      "Train Epoch: 15 [1792/2716 (66%)]\tLoss: 0.099344\n",
      "Train Epoch: 15 [2352/2716 (99%)]\tLoss: 0.096995\n",
      "current lr[2.814749767106563e-05]\n",
      "Valid Epoch: 15 Current Validation Loss: 0.235917\n",
      "Best model obtained! Set Patience = 2\n",
      "trigger times: 0 Best_Val_loss: 0.235917\n",
      "Train Epoch: 16 [0/2716 (0%)]\tLoss: 0.109892\n",
      "Train Epoch: 16 [896/2716 (33%)]\tLoss: 0.096321\n",
      "Train Epoch: 16 [1792/2716 (66%)]\tLoss: 0.098937\n",
      "Train Epoch: 16 [2352/2716 (99%)]\tLoss: 0.078697\n",
      "current lr[2.2517998136852506e-05]\n",
      "Valid Epoch: 16 Current Validation Loss: 0.235566\n",
      "Best model obtained! Set Patience = 2\n",
      "trigger times: 0 Best_Val_loss: 0.235566\n",
      "Train Epoch: 17 [0/2716 (0%)]\tLoss: 0.080308\n",
      "Train Epoch: 17 [896/2716 (33%)]\tLoss: 0.086225\n",
      "Train Epoch: 17 [1792/2716 (66%)]\tLoss: 0.075305\n",
      "Train Epoch: 17 [2352/2716 (99%)]\tLoss: 0.086838\n",
      "current lr[1.8014398509482006e-05]\n",
      "Valid Epoch: 17 Current Validation Loss: 0.233214\n",
      "Best model obtained! Set Patience = 2\n",
      "trigger times: 0 Best_Val_loss: 0.233214\n",
      "Train Epoch: 18 [0/2716 (0%)]\tLoss: 0.093903\n",
      "Train Epoch: 18 [896/2716 (33%)]\tLoss: 0.106028\n",
      "Train Epoch: 18 [1792/2716 (66%)]\tLoss: 0.080948\n",
      "Train Epoch: 18 [2352/2716 (99%)]\tLoss: 0.104039\n",
      "current lr[1.4411518807585605e-05]\n",
      "Valid Epoch: 18 Current Validation Loss: 0.232881\n",
      "Best model obtained! Set Patience = 2\n",
      "trigger times: 0 Best_Val_loss: 0.232881\n",
      "Train Epoch: 19 [0/2716 (0%)]\tLoss: 0.091766\n",
      "Train Epoch: 19 [896/2716 (33%)]\tLoss: 0.081992\n",
      "Train Epoch: 19 [1792/2716 (66%)]\tLoss: 0.084479\n",
      "Train Epoch: 19 [2352/2716 (99%)]\tLoss: 0.087819\n",
      "current lr[1.1529215046068485e-05]\n",
      "Valid Epoch: 19 Current Validation Loss: 0.235287\n",
      "Best model obtained! Set Patience = 2\n",
      "trigger times: 1\n",
      "Train Epoch: 20 [0/2716 (0%)]\tLoss: 0.095522\n",
      "Train Epoch: 20 [896/2716 (33%)]\tLoss: 0.095763\n",
      "Train Epoch: 20 [1792/2716 (66%)]\tLoss: 0.078542\n",
      "Train Epoch: 20 [2352/2716 (99%)]\tLoss: 0.130021\n",
      "current lr[9.223372036854789e-06]\n",
      "Valid Epoch: 20 Current Validation Loss: 0.233321\n",
      "Best model obtained! Set Patience = 2\n",
      "trigger times: 2\n",
      "Early stopping!\n",
      "Start to test process.\n"
     ]
    }
   ],
   "source": [
    "model_1 = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "model_1.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model_1.fc = nn.Linear(2048, num_classes)\n",
    "model_1 = model_1.cuda()\n",
    "optimizer = torch.optim.Adam(model_1.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "model_1 = train_1(epochs, model_1, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece0edc9-783e-4fa6-9c7a-c375e0ca7f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save my CNN model\n",
    "torch.save(model_1.state_dict(), \"../model/model_img1.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
